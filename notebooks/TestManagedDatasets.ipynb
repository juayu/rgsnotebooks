{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/ec2-user/SageMaker/rgs/miniconda/envs/rgsutils/bin/pip install sqlalchemy-redshift\n",
    "!/home/ec2-user/SageMaker/rgs/miniconda/envs/rgsutils/bin/pip install SQLAlchemy\n",
    "\n",
    "# https://www.pythonsheets.com/notes/python-sqlalchemy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import urllib.parse\n",
    "import multiprocessing as mp\n",
    "import uuid \n",
    "from pgdb import connect\n",
    "from iamconnectioninfo import IamConnection\n",
    "from benchmarkloadrunner import update_task_status\n",
    "\n",
    "iamconnectioninfo = IamConnection()\n",
    "conn_string = f'postgresql+pygresql://{urllib.parse.quote_plus(iamconnectioninfo.username)}:{urllib.parse.quote_plus(iamconnectioninfo.password)}@{iamconnectioninfo.hostname_plus_port}/{iamconnectioninfo.db}'\n",
    "\n",
    "managed_schema = ( \n",
    "    {'schema_name': 'tpcds_1gb'},\n",
    "    {'schema_name': 'tpcds_10gb'},\n",
    "    {'schema_name': 'tpcds_100gb'},\n",
    "    {'schema_name': 'tpcds_1tb'},\n",
    "    {'schema_name': 'tpcds_3tb'},\n",
    "    {'schema_name': 'tpcds_10tb'},\n",
    "    {'schema_name': 'tpcds_30tb'},\n",
    "    {'schema_name': 'tpch_1gb'},\n",
    "    {'schema_name': 'tpch_10gb'},\n",
    "    {'schema_name': 'tpch_100gb'},\n",
    "    {'schema_name': 'tpch_1tb'},\n",
    "    {'schema_name': 'tpch_3tb'},\n",
    "    {'schema_name': 'tpch_10tb'},\n",
    "    {'schema_name': 'tpch_30tb'},\n",
    ")\n",
    "\n",
    "working_dir = '/home/ec2-user/SageMaker/derived-tpcds-tpch-benchmarks'\n",
    "tpcds_ddl_file = f'{working_dir}/ddl/tpcds-ddl.sql'\n",
    "tpch_ddl_file = f'{working_dir}/ddl/tpch-ddl.sql'\n",
    "\n",
    "def clean_all_managed_datasets():\n",
    "    engine = sa.create_engine(conn_string)\n",
    "    conn = engine.connect()\n",
    "    trans = conn.begin()\n",
    "    for schema in managed_schema:\n",
    "        statement = f\"\"\"DROP SCHEMA IF EXISTS {schema.get('schema_name')} CASCADE\"\"\"\n",
    "        engine.execute(statement)\n",
    "\n",
    "    trans.commit()\n",
    "    result = conn.execute('SELECT nspname FROM pg_namespace WHERE nspowner=(select usesysid from pg_user where usename=current_user);')\n",
    "    for _r in result:\n",
    "        print(_r)\n",
    "    print('=============')\n",
    "    trans = conn.begin()\n",
    "    for schema in managed_schema:\n",
    "        statement = f\"\"\"CREATE SCHEMA IF NOT EXISTS {schema.get('schema_name')} \"\"\"\n",
    "        engine.execute(statement)\n",
    "\n",
    "    trans.commit()\n",
    "    result = conn.execute('SELECT nspname FROM pg_namespace WHERE nspowner=(select usesysid from pg_user where usename=current_user);')\n",
    "    for _r in result:\n",
    "        print(_r)\n",
    "    conn.close()\n",
    "    \n",
    "def get_sql_from_file(file):\n",
    "    # Open the .sql file\n",
    "    sql_file = open(file,'r')\n",
    "\n",
    "    # Create an empty command string\n",
    "    sql_command = ''\n",
    "\n",
    "    # Iterate over all lines in the sql file\n",
    "    for line in open(file, 'r').read():\n",
    "        # Ignore commented lines\n",
    "        if not line.startswith('--') and line.strip('\\n'):\n",
    "            # Append line to the command string\n",
    "            sql_command += line.strip('\\n')\n",
    "\n",
    "            # If the command string ends with ';', it is a full statement\n",
    "            if sql_command.endswith(';'):\n",
    "                yield sql_command\n",
    "                sql_command = ''\n",
    "                \n",
    "def load_all_ddl():\n",
    "    engine = sa.create_engine(conn_string)\n",
    "    conn = engine.connect()\n",
    "    # Begin transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    for schema in managed_schema:\n",
    "        set_search_path_statement = f\"\"\"SET search_path TO {schema.get('schema_name')}\"\"\"\n",
    "        conn.execute(set_search_path_statement)\n",
    "        if schema.get('schema_name').startswith('tpcds'):\n",
    "            for stmt in get_sql_from_file(tpcds_ddl_file):\n",
    "                conn.execute(stmt)\n",
    "        else:\n",
    "            for stmt in get_sql_from_file(tpch_ddl_file):\n",
    "                conn.execute(stmt)\n",
    "    \n",
    "    trans.commit()\n",
    "    result = conn.execute(\"\"\"SELECT nspname, count(c.*) num_tables FROM pg_class c JOIN pg_namespace n on n.oid = c.relnamespace and n.nspowner = c.relowner where c.relkind='r' and c.relowner=(select usesysid from pg_user where usename=current_user) group by 1 order by 1;\"\"\")\n",
    "    for _r in result:\n",
    "        print(_r)\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def load_managed_dataset(conn, dataset, scale, clean=False):\n",
    "    data_set = dataset\n",
    "    if data_set == 'tpcds':\n",
    "        tables = [\n",
    "            'store_sales', 'catalog_sales', 'web_sales', 'web_returns',\n",
    "            'store_returns', 'catalog_returns', 'call_center', 'catalog_page',\n",
    "            'customer_address', 'customer', 'customer_demographics', 'date_dim',\n",
    "            'household_demographics', 'income_band', 'inventory', 'item',\n",
    "            'promotion', 'reason', 'ship_mode', 'store', 'time_dim', 'warehouse',\n",
    "            'web_page', 'web_site'\n",
    "        ] \n",
    "    elif data_set == 'tpch':\n",
    "        tables = [\n",
    "            'nation', 'region', 'part', 'supplier', 'partsupp', 'customer',\n",
    "            'orders', 'lineitem'\n",
    "        ]\n",
    "        \n",
    "    schema = f'{dataset}_{scale}'\n",
    "    \n",
    "    if clean:\n",
    "        for table in tables:\n",
    "            conn.execute(f'TRUNCATE {schema}.{table}')\n",
    "    \n",
    "    table_queue = mp.JoinableQueue()\n",
    "    for table in tables:\n",
    "        table_queue.put(table)\n",
    "        \n",
    "    processes = []\n",
    "    num_worker_process = 2\n",
    "    task_status = {'type': 'insert', 'sql': {'task_name': f'load_{data_set}', 'task_version': '1.0',\n",
    "                                                 'task_path': '/home/ec2-user/SageMaker/derived-tpcds-tpch-benchmarks/scripts/CleanManagedDatasets.ipynb',\n",
    "                                                 'task_concurrency': num_worker_process, 'task_status': 'inflight', }}\n",
    "    task_uuid = update_task_status(task_status)\n",
    "    for i in range(num_worker_process):\n",
    "        worker_process = mp.Process(target=load_worker,\n",
    "                                    args=(table_queue, data_set, scale, task_uuid),\n",
    "                                    daemon=True,\n",
    "                                    name=f'{data_set}_worker_process_{i}',\n",
    "        )\n",
    "        worker_process.start()\n",
    "        processes.append(worker_process)\n",
    "\n",
    "    table_queue.join()\n",
    "    return task_uuid\n",
    "\n",
    "def clean_orphan_jobs():\n",
    "    pass\n",
    "    # postgres=# delete from task_load_status where task_uuid in (select task_uuid from task_status where task_status='inflight');\n",
    "    # DELETE 2\n",
    "    # postgres=# delete from task_status where task_status='inflight';\n",
    "    # DELETE 1\n",
    "\n",
    "def load_worker(queue, data_set, scale, task_uuid):\n",
    "\n",
    "    while True:\n",
    "        tbl = queue.get()\n",
    "        print('Processing %s (MP: %s) ' % (tbl, mp.current_process().name))\n",
    "\n",
    "        schema = '{}_{}'.format(data_set, scale)\n",
    "\n",
    "        bucket = 'redshift-managed-loads-datasets-us-east-1'\n",
    "        copy_sql = f\"COPY {tbl} FROM 's3://{bucket}/dataset={data_set}/size={scale.upper()}/table={tbl}/{tbl}.manifest' iam_role '{iamconnectioninfo.iamrole}' gzip delimiter '|' COMPUPDATE OFF MANIFEST\"\n",
    "        copy_sql_double_quoted = copy_sql.translate(str.maketrans({\"'\": r\"''\"}))\n",
    "\n",
    "        with connect(dbname='postgres',host='0.0.0.0',user='postgres') as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\n",
    "                f\"INSERT INTO task_load_status(task_uuid,tablename,dataset,status,load_start,querytext) values('{task_uuid}','{tbl}','{schema}','inflight',timezone('utc', now()),'{copy_sql_double_quoted}')\")\n",
    "\n",
    "        with connect(database=iamconnectioninfo.db,\n",
    "                     host=iamconnectioninfo.hostname_plus_port,\n",
    "                     user=iamconnectioninfo.username,\n",
    "                     password=iamconnectioninfo.password) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('set search_path to %s' % (schema))\n",
    "            cursor.execute(copy_sql)\n",
    "            cursor.execute('select pg_last_copy_id()')\n",
    "            query_id = int(\"\".join(filter(str.isdigit,\n",
    "                                          str(cursor.fetchone()))))\n",
    "            cursor.execute('select count(*) from %s' % (tbl))\n",
    "            row_count = int(\"\".join(filter(str.isdigit,\n",
    "                                           str(cursor.fetchone()))))\n",
    "            cursor.execute(\n",
    "                'select count(*) from stv_blocklist where tbl=\\'%s.%s\\'::regclass::oid'\n",
    "                % (schema, tbl))\n",
    "            block_count = int(\"\".join(\n",
    "                filter(str.isdigit, str(cursor.fetchone()))))\n",
    "\n",
    "            \n",
    "\n",
    "        with connect(dbname='postgres',host='0.0.0.0',user='postgres') as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\n",
    "                'UPDATE task_load_status SET status=\\'complete\\',load_end=timezone(\\'utc\\', now()), '\n",
    "                'query_id=%s,rows_d=%s, size_d=%s WHERE tablename=\\'%s\\' and dataset=\\'%s\\''\n",
    "                % (query_id, row_count, block_count, tbl, schema))\n",
    "\n",
    "        queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "('tpcds_1gb',)\n",
      "('tpcds_10gb',)\n",
      "('tpcds_100gb',)\n",
      "('tpcds_1tb',)\n",
      "('tpcds_3tb',)\n",
      "('tpcds_10tb',)\n",
      "('tpcds_30tb',)\n",
      "('tpch_1gb',)\n",
      "('tpch_10gb',)\n",
      "('tpch_100gb',)\n",
      "('tpch_1tb',)\n",
      "('tpch_3tb',)\n",
      "('tpch_10tb',)\n",
      "('tpch_30tb',)\n",
      "('tpcds_100gb', 24)\n",
      "('tpcds_10gb', 24)\n",
      "('tpcds_10tb', 24)\n",
      "('tpcds_1gb', 24)\n",
      "('tpcds_1tb', 24)\n",
      "('tpcds_30tb', 24)\n",
      "('tpcds_3tb', 24)\n",
      "('tpch_100gb', 8)\n",
      "('tpch_10gb', 8)\n",
      "('tpch_10tb', 8)\n",
      "('tpch_1gb', 8)\n",
      "('tpch_1tb', 8)\n",
      "('tpch_30tb', 8)\n",
      "('tpch_3tb', 8)\n"
     ]
    }
   ],
   "source": [
    "# Drop all managed data sets and create empty schema\n",
    "clean_all_managed_datasets()\n",
    "# Load all table DDL\n",
    "load_all_ddl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TPC-DS 1GB\n",
    "engine = sa.create_engine(conn_string)\n",
    "task_uuid = load_managed_dataset(engine, 'tpcds', '1gb', clean=True)\n",
    "task_status = {'type': 'update', 'uuid': task_uuid, 'sql': {'task_status': 'complete', }}\n",
    "update_task_status(task_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TPC-DS 10GB\n",
    "engine = sa.create_engine(conn_string)\n",
    "task_uuid = load_managed_dataset(engine, 'tpcds', '10gb', clean=True)\n",
    "task_status = {'type': 'update', 'uuid': task_uuid, 'sql': {'task_status': 'complete', }}\n",
    "update_task_status(task_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TPC-DS 100GB\n",
    "engine = sa.create_engine(conn_string)\n",
    "task_uuid = load_managed_dataset(engine, 'tpcds', '100gb', clean=True)\n",
    "task_status = {'type': 'update', 'uuid': task_uuid, 'sql': {'task_status': 'complete', }}\n",
    "update_task_status(task_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nation (MP: tpch_worker_process_0) \n",
      "Processing region (MP: tpch_worker_process_1) \n",
      "Processing part (MP: tpch_worker_process_1) \n",
      "Processing supplier (MP: tpch_worker_process_0) \n",
      "Processing partsupp (MP: tpch_worker_process_0) \n",
      "Processing customer (MP: tpch_worker_process_1) \n",
      "Processing orders (MP: tpch_worker_process_1) \n",
      "Processing lineitem (MP: tpch_worker_process_0) \n"
     ]
    }
   ],
   "source": [
    "# Load TPC-H 1GB\n",
    "engine = sa.create_engine(conn_string)\n",
    "task_uuid = load_managed_dataset(engine, 'tpch', '1gb', clean=True)\n",
    "task_status = {'type': 'update', 'uuid': task_uuid, 'sql': {'task_status': 'complete', }}\n",
    "update_task_status(task_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nation (MP: tpch_worker_process_0) \n",
      "Processing region (MP: tpch_worker_process_1) \n",
      "Processing part (MP: tpch_worker_process_0) \n",
      "Processing supplier (MP: tpch_worker_process_1) \n"
     ]
    }
   ],
   "source": [
    "# Load TPC-H 10GB\n",
    "engine = sa.create_engine(conn_string)\n",
    "task_uuid = load_managed_dataset(engine, 'tpch', '10gb', clean=True)\n",
    "task_status = {'type': 'update', 'uuid': task_uuid, 'sql': {'task_status': 'complete', }}\n",
    "update_task_status(task_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TPC-H 100GB\n",
    "engine = sa.create_engine(conn_string)\n",
    "task_uuid = load_managed_dataset(engine, 'tpch', '100gb', clean=True)\n",
    "task_status = {'type': 'update', 'uuid': task_uuid, 'sql': {'task_status': 'complete', }}\n",
    "update_task_status(task_status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_rgsutils",
   "language": "python",
   "name": "conda_rgsutils"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

